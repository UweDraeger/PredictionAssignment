---
title: "PredictionAssignment"
author: "Uwe Draeger"
date: "3/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(knitr)

library(tidyverse)
library(tidymodels)
library(ranger)

library(parallel)
library(doParallel)

```

## Executive Summary

Goal of the project was to create a model to predict correct execution or common
mistakes of barbell-lifting exercises based on measurements from fitness
trackers. A random forest model was created based on the tidymodels package in
R. The model performs well with a likelihood of correct prediction above 99%.\
Consequently, fitness trackers or smartwatches can indeed be used to alert their
users about incorrect execution of certain exercises.

## Data import

Data for this project come from this source:
<http://groupware.les.inf.puc-rio.br/har>.

```{r enable_parallel}

cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)

```

```{r import_data}

# read the data files, set missing data and errors to NA
pml_training <-
  as_tibble(read_csv("pml-training.csv", na = c("#DIV/0!", "NA"))) %>%
  mutate_if(is.character, as.factor)

# to avoid confusion, the "testing" set is called newdata 
pml_newdata <-
  as_tibble(read_csv("pml-testing.csv", na = c("#DIV/0!", "NA"))) %>%
  mutate_if(is.character, as.factor)

```

Two data files are imported and converted to tibbles.

\* pml-training.csv: 19,622 observations of 160 variables\
\* pml-testing.csv: 20 observations of 160 variables

During the import values of "\#DIV/0!" and "NA" are set to NA. Categorical
variables are converted to factors. \
To avoid confusion when cross-validating (where "training" and "testing" are
common terminology) the pml-testing data is named pml_newdata.

## Exploration

The outcome variable "classe" is categorical and thus the classification should
be predicted.

A comparison of variable names in the two datasets shows is a difference.
Training data contains the independent variable "classe" whereas newdata has a
variable called "problem_id" which is identical to the row number. Consequently,
pml_newdata has no outcome variable and the true out-of-sample predictive
ability of the model remains an open question.

A further analysis of pml_newdata shows that many variables consist of missing
values only. It may not be sensible to even start constructing a prediction
model, if in the end it potentially relies on indicators which are not available
when applying the model. This reasoning creates a first exclusion criterion for
variables in the training data set.

The result is a list of 59 variables available as potential predictors.

The training data contains additional variables to identify the observations and
thus can be expected to have no influence on the outcome, like row numbers or
timestamps. Those will be excluded from the model building process as well. An
interesting question is whether the user identifier should be included as a
predictor. There may be individual differences in the execution that are typical
for user A but not for B, C, etc. If the goal is to predict for the given users
only then it should be helpful to include the user name as a predictor, i.e.
have a different model for each user - the fact that the out-of-sample data
contains identical user names is tempting... However, since the real goal of the
study is likely to be able to predict the classification for a typical user, the
user name is excluded from the set of predictors in this analysis.

52 potential predictor variables survive with at least 3,200 observations in
each category. The number of observations per execution shows no major
imbalance.

```{r exploration}

# find all variables with usable data in newdata, remove columns with NA only
vars_with_data <- pml_newdata %>% 
  select(which(colMeans(is.na(.)) == 0))
prednames <- colnames(vars_with_data)

# reduce training set to variables actually available in test set
# predname[60] is problem_id (copy of X1) -> remove
# predname[1:7] - descriptive variables -> exclude from prediction
# add back the independent variable 
train <- pml_training %>% 
  select(prednames[-60]) %>% 
  select(-(X1:num_window)) %>%
  mutate(classe = pml_training$classe)

# table results
kable(table(pml_training$classe))

```

## Cross validation

The processed dataset (train) is split into a training and a test set using a
ratio of 3 to 1. The training set will be used to build and tune the model. The
test set then allows to assess the forecasting ability on data that has not been
used to build and tune the predicting model.

```{r cross_validation}

# set seed as there is randomness in assigning cases to train or test
set.seed(12345)

# split train and test, create cross validation object 
train_split <- initial_split(train, prop = 0.75)

pml_train <- training(train_split)
pml_test <- testing(train_split)

# create cross validation object to be used later in tuning
train_cv <- vfold_cv(train)

```

## Model building

### Define a recipe

The tidymodels package allows to define a "recipe". Such recipes start with
basic information about the model by defining the data and the roles of
variables as outcomes, predictors, or not-to-be-used descriptors. The recipe
also contains preprocessing steps to further transform the variables.

Given that all predictors are numerical but on different scales we normalize
(center plus scale) them to bring the values to a comparable scale.

The recipe will automatically be applied to the test data when validating the
model later on.

```{r recipe}

# set up formula and 
# preprocessing: normalize numeric data 
pml_recipe <-
  recipe(classe ~ ., data = pml_train) %>%
  step_normalize(all_numeric()) 

```

### Specify the model

A classification model has to be used to answer the question if the
weight-lifting exercise had been performed correctly or what type of mistake has
been made.

The model used is of the "random forest" type. Such models tend to exhibit high
accuracy, especially in situation with many more observations then predictors.
Internally, random samples of the observations are taken via bootstrapping.
Additionally also the variables are randomly sampled via bootstrapping. Many
trees are build and a voting algorithm is applied to define the optimal decision
tree.\
Disadvantages of random forest models are their high requirements of computing
power / slow speed, some lack of in-depth interpretability, and a tendency to
overfit. A careful assessment in an out-of-sample context is recommended.

There are three "hyperparameters" than can be used to tune the model.

1.  Number of predictors (mtry)
2.  Number of trees (trees)
3.  Minimum number of data points in a node required to split further (min_n).\
    This parameter is not used in tuning the model, i.e. left at the default
    value of 1.

The model will be tuned by simulating in-sample performance for a grid of
different mtry and trees parameters.

```{r model}

# spec the model to be random forest 
# set arguments mtry and trees for tuning 
# use ranger implementation of random forest to classify
pml_model <- rand_forest() %>%
  set_args(mtry = tune(), trees = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

pml_model

```

### Build the workflow

A workflow combines recipe and model.

```{r workflow}

# build the workflow by combining recipe and model
pml_workflow <- workflow() %>%
  add_recipe(pml_recipe) %>%
  add_model(pml_model)

pml_workflow

```

### Parameter tuning

As outlined above, some values of the mtry and trees parameters are varied and
the resulting predictions are evaluated in-sample on the training set.

The usual recommendation for mtry is sqrt(number of predictors) which would be
roughly 7 in the case of 52 predictors. The default value for trees is 500.

```{r tuning}

# tune mtry and trees parameters
pml_grid <- expand.grid(mtry = c(2, 5, 7), trees = c(50, 100, 500))
pml_tune_results <- pml_workflow %>%
  tune_grid(
    resamples = train_cv,
    grid = pml_grid,
    metrics = metric_set(accuracy, sens, spec)
    )

results <- pml_tune_results %>% 
        collect_metrics()

results

# next (commented) lines of code would automatically use the best parameter set, 
# which gets the process closer to unsupervised learning

# tuning_result <- pml_tune_results %>% select_best(metric = "accuracy")
# pml_workflow <- pml_workflow %>% finalize_workflow(tuning_result)

```

Tuning was performed using mtry = c(2, 5, 7) and trees = c(50, 100, 500).

The ad-hoc expectation was that the inclusion of more predictors and the
creation of more trees increases in-sample accuracy. However, even with the
smallest parameters (i.e. mtry = 2 and trees = 50) the accuracy was above 99%,
equally high were sensitivity and specificity. Any increase in the parameters
will, on the other hand, increase the amount of potential over-fitting.

```{r tuning_result}

# create tibble with accuracy data
accuracies <- results %>% 
        filter(.metric == "accuracy") %>% 
        select(mtry, trees, mean, std_err) %>% 
        arrange(mtry, trees)

accuracies

# make a plot
ggplot(data = accuracies, aes(x = mtry, y = mean, color = as_factor(trees))) +
        geom_point(position = position_jitter(width = .05, height = 0.005)) +
        labs(title = "Jittered plot of accuracies", 
             color = "Parameter: trees") + 
        ylim(0.75, 1) + 
        ylab("Mean Accuracy") +
        scale_x_continuous(breaks = c(2,5,7)) +
        xlab("Parameter: mtry")

```

The plot makes it obvious that there is hardly any difference in the accuracies
achieved.\
The final model will be run with mtry = 7 (as this is close to the rule of
thumb) and trees = 100 (as a compromise between speed of computation and
precision).

### Add selected parameters to workflow

With the selected set of model parameters the final model can be defined and
combined with the recipe will form the workflow.

```{r finalize}

# set selected parameter values in final model
pml_final_model <- rand_forest() %>%
  set_args(mtry = 7, trees = 100) %>%
  set_engine("ranger") %>%
  set_mode("classification")

pml_final_model

#combine recipe with final model 
pml_workflow <- workflow() %>%
  add_recipe(pml_recipe) %>%
  add_model(pml_final_model)

pml_workflow

```

### Apply model to test

In a last step the workflow will be applied to the test set from
cross-validation (pml_test).\
The predictions are generated, accuracy and confusion matrix are given below.

```{r apply_to_test}

# evaluate model on validation set
pml_fit <- pml_workflow %>%
  last_fit(train_split)

# calculate metrics (interested in accuracy)
train_perf <- pml_fit %>%
  collect_metrics()
kable(train_perf)

# predict classe in training set
validat_pred <- pml_fit %>%
  collect_predictions()

validat_pred %>%
  conf_mat(truth = classe, estimate = .pred_class)

```

The result (99.51%) confirms the high accuracy in the training set (99.55%).

Given the fact that pml-training and pml-testing set were likely generated from
the same individuals, there is a fair chance for a significant drop in accuracy
when other individuals are measured.
