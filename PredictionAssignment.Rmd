---
title: "PredictionAssignment"
author: "Uwe Draeger"
date: "3/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)

library(tidyverse)
library(readr)
library(tidymodels)
library(ranger)

library(parallel)
library(doParallel)

```

## Executive Summary

tidymodels rather than caret
random forest to classify

## Data import

```{r enable_parallel}

cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)

```


```{r import_data}

# read the data files, set missing data and errors to NA
pml_training <-
        as_tibble(read_csv("pml-training.csv", na = c("#DIV/0!", "NA"))) %>%
        mutate_if(is.character, as.factor)

# to avoid confusion, the "testing" set is called newdata 
pml_newdata <-
        as_tibble(read_csv("pml-testing.csv", na = c("#DIV/0!", "NA"))) %>%
        mutate_if(is.character, as.factor)

```



## Exploration

An analysis of the out-of-sample data (pml-testing) shows that quite a few variables consist of missing values only. It may not be sensible to even start constructing a prediction model, if in the end it potentially relies on indicators which are not available when applying the model. This reasoning creates a first exclusion criterion for variables in the training data set. 

The result is a list of 59 variables available as predictors. 


The training data contains additional variables to identify the observations and can be expected to have no influence on the outcome, like row numbers or timestamps. Those will be excluded from the model building process as well. 
An interesting question is whether the user identifier should be included as a predictor. There may be individual differenes in the execution that are typical for user A but not for B, C, etc. If the goal is to predict the users only then it should be helpful to predict a different model for each user - the fact that the out-of-sample data contains identical user names is tempting...
However, since the real goal is likely to predict for a typical user, the user name is excluded from the set of predictors.

52 variables survive with information on six different users with at least 2,600 observations each. 
The number of observations per user in each class of execution (classe - the independent variable) is also large, with a minimum of almost 500 observation per bucket, and relatively evenly split.

```{r users}

table(pml_training$classe)
table(pml_training$user_name)
table(pml_training$user_name, pml_training$classe)

```


## Cross validation

Although there is a set of new data for the prediction, it seems prudent to create a validation set to enable some estimation of model quality and expected prediction error. 



## Model building

Type of final model: Classification - has the exercise been performed as specified or what type of mistake has been made.

Tuning was performed using mtry = c(6,7,8) trees = c(100, 200, 500) - almost no differences in accuracy, sens or spec = use mtry = 6 and trees = 100

        mtry trees .metric  .estimator  mean     n  std_err .config             
        <dbl> <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>               
 1     6   100 accuracy multiclass 0.996    10 0.000479 Preprocessor1_Model1
 2     6   100 sens     macro      0.995    10 0.000565 Preprocessor1_Model1
 3     6   100 spec     macro      0.999    10 0.000124 Preprocessor1_Model1
 
   mtry trees .metric  .estimator  mean     n  std_err .config             
  <dbl> <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>               
1     5    10 accuracy multiclass 0.991    10 0.00106  Preprocessor1_Model9
2     5    10 sens     macro      0.990    10 0.00121  Preprocessor1_Model9
3     5    10 spec     macro      0.998    10 0.000271 Preprocessor1_Model9
 

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
