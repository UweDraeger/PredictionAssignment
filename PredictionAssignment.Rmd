---
title: "PredictionAssignment"
author: "Uwe Draeger"
date: "3/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(knitr)

library(tidyverse)
library(tidymodels)
library(ranger)

library(parallel)
library(doParallel)

```

## Executive Summary

tidymodels rather than caret random forest to classify

## Data import

```{r enable_parallel}

cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)

```

```{r import_data}

# read the data files, set missing data and errors to NA
pml_training <-
  as_tibble(read_csv("pml-training.csv", na = c("#DIV/0!", "NA"))) %>%
  mutate_if(is.character, as.factor)

# to avoid confusion, the "testing" set is called newdata 
pml_newdata <-
  as_tibble(read_csv("pml-testing.csv", na = c("#DIV/0!", "NA"))) %>%
  mutate_if(is.character, as.factor)

```

Two data files are imported and converted to tibbles.

\* pml-training.csv: 19,622 observations of 160 variables\
\* pml-testing.csv: 20 observations of 160 variables

During the import values of "\#DIV/0!" and "NA" to NA. Categorical variables are
converted to factors.\
To avoid confusion when cross-validating (where "training" and "testing" are
common terminology) pml-testing is renamed to pml-newdata.

## Exploration

The outcome variable "classe" is categorical and thus a classification should be
predicted.

A comparison of variable names in the two datasets shows that there is a
difference. Training data contains the independent variable "classe" whereas
newdata does have a variable called "problem_id" which is identical to the row
number. Consequently, pml-newdata has no outcome variable and the true
out-of-sample predictive ability of the model remains an open question.

A further analysis of pml-newdata shows that many variables consist of missing
values only. It may not be sensible to even start constructing a prediction
model, if in the end it potentially relies on indicators which are not available
when applying the model. This reasoning creates a first exclusion criterion for
variables in the training data set.

The result is a list of 59 variables available as predictors.

The training data contains additional variables to identify the observations and
thus can be expected to have no influence on the outcome, like row numbers or
timestamps. Those will be excluded from the model building process as well. An
interesting question is whether the user identifier should be included as a
predictor. There may be individual differences in the execution that are typical
for user A but not for B, C, etc. If the goal is to predict for the given users
only then it should be helpful to include the user name as a predictor, i.e.
have a different model for each user - the fact that the out-of-sample data
contains identical user names is tempting... However, since the real goal is
likely to predict for a typical user, the user name is excluded from the set of
predictors in this analysis.

52 variables survive with information on six different users with at least 2,600
observations each. The number of observations per user in each class of
execution (classe - the independent variable) is also large, with a minimum of
almost 500 observation per bucket, and relatively evenly split.

```{r exploration}

# find all variables with usable data in testing (i.e. remove all cols with NA only)
vars_with_data <- pml_newdata %>% 
  select(which(colMeans(is.na(.)) == 0))
prednames <- colnames(vars_with_data)

# reduce training set to variables actually available in test set
# predname[60] is problem_id (copy of X1) -> remove
# predname[1:7] - descriptive variables -> exclude from prediction
# add back the independent variable 
train <- pml_training %>% 
  select(prednames[-60]) %>% 
  select(-(X1:num_window)) %>%
  mutate(classe = pml_training$classe)

# table results
kable(table(pml_training$classe))
kable(table(pml_training$user_name))
kable(table(pml_training$user_name, pml_training$classe))

```

## Cross validation

The initial dataset (pml_testing) is split into a training and a test set using
a ratio of 3 to 1. The training set will be used to build and tuning the model.
The test set then allows to assess the forecasting ability on data that has not
been used to build and tune.

```{r cross_validation}

# set seed as there is randomness in assigning cases to train or test
set.seed(12345)

# split train and test, create cross validation object 
train_split <- initial_split(train, prop = 0.75)

pml_train <- training(train_split)
pml_test <- testing(train_split)

# create cross validation object to be used later in tuning
train_cv <- vfold_cv(train)

```

## Model building

### Define a recipe

The tidymodels package allows to define a "recipe". Such recipes start with
basic information about the model by defining the data and the roles of
variables as outcomes, predictors, or not-to-be-used descriptors. The recipe
also provides preprocessing steps to transform the variables.

Given that all predictors are numerical but on different scales we only
normalize (center plus scale) them to bring the values to a comparable scale.

The recipe will automatically be applied to the test data when validating the
model.

```{r recipe}

# set up formula and 
# preprocessing: normalize numeric data 
pml_recipe <-
  recipe(classe ~ ., data = pml_train) %>%
  step_normalize(all_numeric()) 

```

### Specify the model

Type of final model: Classification - has the weight-lifting exercise been
performed as specified or what type of mistake has been made.

The model used is of the "random forest" type. Such models tend to exhibit high
accuracy, especially in situation with many more observations then predictors.
Internally, random samples of the observations are taken via bootstrapping.
Additionally also the variables are randomly sampled via bootstrapping. Many
trees are build and a voting algorithm is applied to define the optimal decision
tree.\
Disadvantages of random forest models are their high requirements of computing
power / slow speed, some lack of in-depth interpretability, and a tendency to
overfit. A careful assessment in an out-of-sample context is recommended.

There are three "hyperparameters" than can be used to tune the model.

1.  Number of predictors (mtry)
2.  Number of trees (trees)
3.  Minimum number of data points in a node required to split further (min_n).\
    This parameter is not used in tuning the model, i.e. left at the default
    value of 1.

The model will be tuned by simulating in-sample performance for a range of mtry
and trees parameters.

```{r model}

# spec the model to be random forest 
# set arguments mtry and trees for tuning 
# use ranger implementation of random forest to classify
pml_model <- rand_forest() %>%
  set_args(mtry = tune(), trees = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

```

### Build the workflow

A workflow combines a recipe with a model. Nothing is computed yet.

```{r workflow}

# build the workflow by combining recipe and model
pml_workflow <- workflow() %>%
  add_recipe(pml_recipe) %>%
  add_model(pml_model)

```

### Parameter tuning

As outlined above, changes to mtry and trees parameters are to be changed and
the quality of the resulting predictions are evaluated in-sample on the train
set.

The usual recommendation for mtry is sqrt(number of predictors) which would be
roughly 7 in the case of 52 predictors. The default value for trees is 500.

```{r tuning}

# tune mtry and trees parameters
pml_grid <- expand.grid(mtry = c(2, 5, 7), trees = c(50, 100, 500))
pml_tune_results <- pml_workflow %>%
  tune_grid(
    resamples = train_cv,
    grid = pml_grid,
    metrics = metric_set(accuracy, sens, spec)
    )

results <- pml_tune_results %>% 
        collect_metrics()

# next (commented) lines of code would use best parameter set, getting the process # closer to unsupervised learning

# tuning_result <- pml_tune_results %>% select_best(metric = "accuracy")
# pml_workflow <- pml_workflow %>% finalize_workflow(tuning_result)
```

Tuning was performed using mtry = c(2, 5, 7) and trees = c(50, 100, 500).

The ad-hoc expectation was that the inclusion of more predictors and the
creation of more trees increases in-sample accuracy. However, even with the
smallest parameters (i.e. mtry = 2 and trees = 50) the accuracy was above 99%,
equally high were sensitivity and specificity. Any increase in the parameters
will, on the other hand, increase the amount of potential over-fitting.

```{r tuning_result}

# create tibble with accuracy data
accuracies <- results %>% 
        filter(.metric == "accuracy") %>% 
        select(mtry, trees, mean, std_err) %>% 
        arrange(mtry, trees)
accuracies

# make a plot
ggplot(data = accuracies, aes(x = mtry, y = mean, color = as_factor(trees))) +
        geom_point(position = position_jitter(width = .05, height = 0.005)) +
        labs(title = "Jittered plot of accuracies", 
             caption = "mtry parameter") + 
        ylim(0.75, 1) + 
        scale_x_continuous(breaks = c(2,5,7))

```

The plot makes it obvious that there is hardly any difference in the accuracies
achieved. \
The final model will be run with mtry = 7 (as this is close to the rule of
thumb) and trees = 100 (as a compromise between speed of computation and
precision).

### Add selected parameters to workflow

```{r finalize}

# set selected parameter values in final model
pml_final_model <- rand_forest() %>%
  set_args(mtry = 7, trees = 100) %>%
  set_engine("ranger") %>%
  set_mode("classification")

#combine recipe with final model 
pml_workflow <- workflow() %>%
  add_recipe(pml_recipe) %>%
  add_model(pml_final_model)

```

### Apply model to test

In a last step the model will be applied to the test set (pml_test).

```{r apply_to_test}

# evaluate model on validation set
pml_fit <- pml_workflow %>%
  last_fit(train_split)

# calculate metrics (interested in accuracy)
train_perf <- pml_fit %>%
  collect_metrics()
kable(train_perf)

# predict classe in training set
validat_pred <- pml_fit %>%
  collect_predictions()

validat_pred %>%
  conf_mat(truth = classe, estimate = .pred_class)

```

The result (99.51%) confirms the high accuracy in the training set (99.55%).

Given the fact that training and testing set were generated from the same
individuals, there is a fair chance for a significant drop in accuracy when
other individuals are measured.
